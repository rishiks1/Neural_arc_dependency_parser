{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from collections import defaultdict, deque\n",
        "from torch.utils.data import Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "class DependencyTree:\n",
        "    def __init__(self, tokens):\n",
        "        self.tokens = tokens\n",
        "        self.size = len(tokens)\n",
        "        self.heads = [-1] * self.size  # Initialize heads with -1\n",
        "        self.labels = [None] * self.size\n",
        "\n",
        "class Configuration:\n",
        "    def __init__(self, tree):\n",
        "        self.tree = tree\n",
        "        self.stack = [0]  # Root token is always at index 0\n",
        "        self.buffer = deque(range(1, tree.size))\n",
        "\n",
        "    def shift(self):\n",
        "        if self.buffer:\n",
        "            self.stack.append(self.buffer.popleft())\n",
        "\n",
        "    def left_arc(self):\n",
        "        if len(self.stack) > 1:\n",
        "            dep = self.stack.pop(-2)\n",
        "            self.tree.heads[dep] = self.stack[-1]\n",
        "\n",
        "    def right_arc(self):\n",
        "        if len(self.stack) > 1:\n",
        "            dep = self.stack.pop()\n",
        "            self.tree.heads[dep] = self.stack[-1]\n",
        "\n",
        "class DependencyDataset(Dataset):\n",
        "    def __init__(self, trees, word_vocab, label_vocab):\n",
        "        self.trees = trees\n",
        "        self.word_vocab = word_vocab\n",
        "        self.label_vocab = label_vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.trees)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tree = self.trees[idx]\n",
        "        words = [self.word_vocab[token['form'].lower()] for token in tree.tokens]\n",
        "        heads = [token['head'] for token in tree.tokens]\n",
        "        labels = [self.label_vocab[token['deprel']] for token in tree.tokens]\n",
        "        return torch.tensor(words, dtype=torch.long), torch.tensor(heads, dtype=torch.long), torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "def custom_collate_fn(batch):\n",
        "    words, heads, labels = zip(*batch)\n",
        "    words_padded = pad_sequence([seq.clone().detach() for seq in words], batch_first=True, padding_value=0)\n",
        "    heads_padded = pad_sequence([seq.clone().detach() for seq in heads], batch_first=True, padding_value=-1)  # Use an ignore index\n",
        "    labels_padded = pad_sequence([seq.clone().detach() for seq in labels], batch_first=True, padding_value=-1)  # Use an ignore index\n",
        "    return words_padded, heads_padded, labels_padded\n",
        "\n",
        "def parse_conllu(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        data = file.read().strip().split('\\n\\n')\n",
        "    trees = []\n",
        "    for sentence in data:\n",
        "        tokens = []\n",
        "        for line in sentence.split('\\n'):\n",
        "            if line.startswith('#') or '-' in line.split('\\t')[0] or '.' in line.split('\\t')[0]:\n",
        "                continue  # Skip comment lines and non-integer token IDs\n",
        "            parts = line.split('\\t')\n",
        "            if len(parts) == 10:\n",
        "                tokens.append({\n",
        "                    'id': int(parts[0]),\n",
        "                    'form': parts[1],\n",
        "                    'lemma': parts[2],\n",
        "                    'upos': parts[3],\n",
        "                    'xpos': parts[4],\n",
        "                    'feats': parts[5],\n",
        "                    'head': int(parts[6]),\n",
        "                    'deprel': parts[7],\n",
        "                    'deps': parts[8],\n",
        "                    'misc': parts[9]\n",
        "                })\n",
        "        trees.append(DependencyTree(tokens))\n",
        "    return trees\n",
        "\n",
        "def build_vocab(trees):\n",
        "    word_vocab = defaultdict(lambda: len(word_vocab))\n",
        "    label_vocab = defaultdict(lambda: len(label_vocab))\n",
        "    for tree in trees:\n",
        "        for token in tree.tokens:\n",
        "            _ = word_vocab[token['form'].lower()]\n",
        "            _ = label_vocab[token['deprel']]\n",
        "    return dict(word_vocab), dict(label_vocab)\n"
      ],
      "metadata": {
        "id": "9QM4BfzgQR6v"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "class DependencyParserModel(nn.Module):\n",
        "    def __init__(self, vocab_size, label_size, embedding_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=1, bidirectional=True)\n",
        "        self.head_predictor = nn.Linear(hidden_dim * 2, vocab_size)  # Adjust output dimensions if needed\n",
        "        self.label_predictor = nn.Linear(hidden_dim * 2, label_size)\n",
        "\n",
        "    def forward(self, words):\n",
        "        embeds = self.embed(words)\n",
        "        lstm_out, _ = self.lstm(embeds)\n",
        "        heads = self.head_predictor(lstm_out).squeeze()\n",
        "        labels = self.label_predictor(lstm_out)\n",
        "        return heads, labels  # Directly return logits without applying softmax\n",
        "\n",
        "def train_and_evaluate(model, data_loader, optimizer, criterion_head, criterion_label, epoch_count):\n",
        "    for epoch in range(epoch_count):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for words, true_heads, true_labels in data_loader:\n",
        "            pred_heads, pred_labels = model(words)\n",
        "\n",
        "            loss_heads = criterion_head(pred_heads.transpose(1, 2), true_heads)\n",
        "            loss_labels = criterion_label(pred_labels.transpose(1, 2), true_labels)\n",
        "\n",
        "            loss = loss_heads + loss_labels\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f'Epoch {epoch+1}: Loss {total_loss / len(data_loader)}')\n",
        "\n",
        "def evaluate_model(model, data_loader):\n",
        "    model.eval()\n",
        "    total_correct_heads = 0\n",
        "    total_correct_labels = 0\n",
        "    total_tokens = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for words, true_heads, true_labels in data_loader:\n",
        "            pred_heads, pred_labels = model(words)\n",
        "\n",
        "            # Assuming the output dimensions need adjusting:\n",
        "            pred_heads = pred_heads.argmax(-1)\n",
        "            pred_labels = pred_labels.argmax(-1)\n",
        "\n",
        "            correct_heads = (pred_heads == true_heads) & (true_heads != -1)  # Ignore padding\n",
        "            correct_labels = (pred_labels == true_labels) & (true_labels != -1) & (pred_heads == true_heads)  # Correct label at correct head\n",
        "\n",
        "            total_correct_heads += correct_heads.sum().item()\n",
        "            total_correct_labels += correct_labels.sum().item()\n",
        "            total_tokens += (true_heads != -1).sum().item()  # Count non-padded tokens\n",
        "\n",
        "    uas = total_correct_heads / total_tokens\n",
        "    las = total_correct_labels / total_tokens\n",
        "    return uas, las\n",
        "\n",
        "def train_and_evaluate_language(language, model, optimizer, criterion_head, criterion_label, epoch_count=1, batch_size=10):\n",
        "    train_trees = parse_conllu(f'{language}-ud-train.conllu')\n",
        "    dev_trees = parse_conllu(f'{language}-ud-dev.conllu')\n",
        "    test_trees = parse_conllu(f'{language}-ud-test.conllu')\n",
        "\n",
        "    word_vocab, label_vocab = build_vocab(train_trees + dev_trees + test_trees)\n",
        "    train_dataset = DependencyDataset(train_trees, word_vocab, label_vocab)\n",
        "    dev_dataset = DependencyDataset(dev_trees, word_vocab, label_vocab)\n",
        "    test_dataset = DependencyDataset(test_trees, word_vocab, label_vocab)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
        "    dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn)\n",
        "\n",
        "    train_and_evaluate(model, train_loader, optimizer, criterion_head, criterion_label, epoch_count)\n",
        "\n",
        "    dev_uas, dev_las = evaluate_model(model, dev_loader)\n",
        "    print(f'{language.capitalize()} Development Data - UAS: {dev_uas:.4f}, LAS: {dev_las:.4f}')\n",
        "\n",
        "    test_uas, test_las = evaluate_model(model, test_loader)\n",
        "    print(f'{language.capitalize()} Test Data - UAS: {test_uas:.4f}, LAS: {test_las:.4f}')\n",
        "\n",
        "def main():\n",
        "    languages = ['en_ewt', 'es_ancora']\n",
        "    for language in languages:\n",
        "        print(f'Training and evaluating on {language} dataset...')\n",
        "\n",
        "        train_trees = parse_conllu(f'{language}-ud-train.conllu')\n",
        "        dev_trees = parse_conllu(f'{language}-ud-dev.conllu')\n",
        "        test_trees = parse_conllu(f'{language}-ud-test.conllu')\n",
        "\n",
        "        word_vocab, label_vocab = build_vocab(train_trees + dev_trees + test_trees)\n",
        "\n",
        "        model = DependencyParserModel(vocab_size=len(word_vocab), label_size=len(label_vocab), embedding_dim=100, hidden_dim=256)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "        criterion_head = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "        criterion_label = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "\n",
        "        train_and_evaluate_language(language, model, optimizer, criterion_head, criterion_label, epoch_count=1)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "wzu8YGlwSW2B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a943092-db15-405d-dc14-806b95c2ef5c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training and evaluating on en_ewt dataset...\n",
            "Epoch 1: Loss 4.927411218468412\n",
            "En_ewt Development Data - UAS: 0.1352, LAS: 0.1087\n",
            "En_ewt Test Data - UAS: 0.1393, LAS: 0.1089\n",
            "Training and evaluating on es_ancora dataset...\n",
            "Epoch 1: Loss 5.007658829965985\n",
            "Es_ancora Development Data - UAS: 0.0706, LAS: 0.0515\n",
            "Es_ancora Test Data - UAS: 0.0782, LAS: 0.0564\n"
          ]
        }
      ]
    }
  ]
}